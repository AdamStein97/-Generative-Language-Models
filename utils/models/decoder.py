import tensorflow as tf
from utils.models.lstm import LSTM
import numpy as np

class Decoder():
    def __init__(self, layers, hidden_dim, decode_steps, dtype, decoder_dropout):
        """
        Decoder class that generates a sequence based on a latent variable z
        :param layers: Number of layers of LSTM for the decoder
        :param hidden_dim: Hidden dimension for LSTM
        :param decode_steps: Number of outputs generated by the decoder
        :param dtype: Datatype of the archiecture
        """
        self.model = LSTM(layers, hidden_dim, False, 0,  False, dtype, "dec")
        self.max_steps = decode_steps
        self.augment = False
        self.attention = False
        self.decoder_dropout = decoder_dropout

    def forward(self, z, state):
        """
        Forward pass of the network
        :param z: Vector z to be decoded [1, batch_size, encoder_output_dim]
        :param state: State from the encoder architecture is initial state of decoder
        :return: Decoded output [batch_size, sequence, hidden_dim]
        """
        outputs = []
        input = z
        input_state = state
        for i in range(self.max_steps):
            if self.augment:
                #if augment on - concat augment at each time step
                input = tf.concat([input, self.augement_tensor], -1)
            if self.attention:
                #if attention on - concat calculated attention at each timestep
                attention_value = tf.expand_dims(self.attention_layers[i], 0)
                input = tf.concat([input, attention_value], -1)
            #LSTM needs list as sequence
            input = tf.unstack(input)
            if i == 0 and state == None:
                input_state = self.model.start_state(tf.shape(input[0])[0])  # state
            (output, state) = self.model.forward(input, input_state, None)
            c, h = state[-1] #state from last layer of LSTM
            outputs.append(h) #keep track of the outputs at each time step
            input_with_dropout = self._dropout(h)
            input = tf.expand_dims(input_with_dropout, 0) #expand dim for next time step input
            input_state = state #Keep the output state from the last run
        #convert outputs to tensor of shape [batch_size, 1, decoder_hidden_dim]
        outputs = tf.transpose(tf.stack(outputs), [1,0,2])
        return outputs

    def _dropout(self, h):
        batch_size = h.get_shape().as_list()[0]
        hidden_dim = h.get_shape().as_list()[1]
        unstacked_h = tf.unstack(h)
        for i in range(batch_size):
            t = np.random.random()
            if (t < self.decoder_dropout):
                zero_float_vec = tf.zeros(hidden_dim)
                unstacked_h[i] = zero_float_vec
        return tf.stack(unstacked_h)

    def set_augment_each_time_step(self, augement_tensor):
        """
        Sets up augmentation allowing a tensor to be concatenated at each time step of the decode to hold information
        :param augement_tensor: [1, batch_size, augment_size]
        """
        self.augment = True
        self.augement_tensor = augement_tensor

    def forward_attention(self, attention_context):
        """
        Calculates the attention vector based on the attention context
        :param attention_context: Often the output from the encoder
        """
        self.attention_layers = []
        for i in range(self.max_steps):
            # TODO: Double transpose expensive - try and remove
            softmaxed_weights = tf.nn.softmax(self.weights[i])
            #multiple weights by input and then perform a reduced sum create attention vector
            m = tf.multiply(softmaxed_weights,tf.transpose(attention_context))
            layer = tf.reduce_sum(m , -1)
            layer = tf.transpose(layer)
            #linear layer to reduce dimension of attention vector to set size
            attention = self.attention_reduce_layer.apply(layer)
            #different attention vector for each time step
            self.attention_layers.append(attention)


    def set_up_attention(self, dim):
        """
        Adds attention to the decoder
        :param dim: Dimension of attention vector
        """
        self.attention = True
        self.attention_dim = dim
        for i in range(self.max_steps):
            #TODO: maybe change this initialisation - needs researching what's best
            #weight to multiply each input by, different weights for each time step
            initalise = [1/self.max_steps for i in range(self.max_steps)]
            self.weights = [tf.Variable(initalise, trainable=True, dtype=tf.float32) for i in range(self.max_steps)]
        # linear layer to reduce dimension of attention vector to set size
        self.attention_reduce_layer = tf.layers.Dense(self.attention_dim)
